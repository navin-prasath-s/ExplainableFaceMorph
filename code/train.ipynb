{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:06:25.871808800Z",
     "start_time": "2025-04-28T12:06:25.842115200Z"
    }
   },
   "id": "81722a0f4dfad3ee"
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "data": {
      "text/plain": "'../models'"
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"../dataset\" \n",
    "MODEL_DIR = \"../models\"\n",
    "torch.hub.set_dir(MODEL_DIR)\n",
    "torch.hub.get_dir()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:06:25.871808800Z",
     "start_time": "2025-04-28T12:06:25.845137Z"
    }
   },
   "id": "8eb0781c4806703a"
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fake_segments': 0, 'real_segments': 1}\n",
      "163 41\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 224\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=DATA_DIR)\n",
    "print(full_dataset.class_to_idx)\n",
    "\n",
    "indices = list(range(len(full_dataset)))\n",
    "random.shuffle(indices)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "train_indices = indices[:train_size]\n",
    "test_indices = indices[train_size:]\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "print(len(train_indices), len(test_indices))\n",
    "\n",
    "train_dataset.dataset.transform = train_transform\n",
    "test_dataset.dataset.transform = test_transform\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:06:25.871808800Z",
     "start_time": "2025-04-28T12:06:25.852287700Z"
    }
   },
   "id": "8e3bbd49f1f4c6b9"
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "model = torch.hub.load(f\"{MODEL_DIR}/pytorch_vision_v0.10.0\", 'mobilenet_v2', source=\"local\")\n",
    "model.classifier[1] = nn.Linear(32, 2)\n",
    "model.classifier[0] = torch.nn.Dropout(p=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:07:53.985787500Z",
     "start_time": "2025-04-28T12:07:53.947770900Z"
    }
   },
   "id": "8f0be14c6647fa2b"
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2223938 2223938\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(num_params, num_trainable_params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:07:54.100988900Z",
     "start_time": "2025-04-28T12:07:54.091252400Z"
    }
   },
   "id": "dd2fbf9322221776"
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "data": {
      "text/plain": "MobileNetV2(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU6(inplace=True)\n    )\n    (1): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (4): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=False)\n    (1): Linear(in_features=32, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features = nn.Sequential(*list(model.features.children())[:5])\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:07:54.243822500Z",
     "start_time": "2025-04-28T12:07:54.237289300Z"
    }
   },
   "id": "df85fb864f574d44"
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25858 25858\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(num_params, num_trainable_params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:07:54.404504200Z",
     "start_time": "2025-04-28T12:07:54.397579700Z"
    }
   },
   "id": "e76184a257a72404"
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [],
   "source": [
    "# for param in model.features.parameters():\n",
    "#     param.requires_grad = False\n",
    "# \n",
    "# for param in model.features[-3:].parameters():\n",
    "#     param.requires_grad = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:07:54.936117100Z",
     "start_time": "2025-04-28T12:07:54.932698900Z"
    }
   },
   "id": "c74bc6b8282d0802"
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25858 25858\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(num_params, num_trainable_params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:07:55.104792100Z",
     "start_time": "2025-04-28T12:07:55.097901300Z"
    }
   },
   "id": "a7c2d7c3c3717034"
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total_train = 0\n",
    "\n",
    "        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\", leave=False)\n",
    "        for images, labels in train_loop:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        avg_train_loss = running_loss / total_train\n",
    "        train_accuracy = train_correct / total_train\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        total_val = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\", leave=False)\n",
    "            for images, labels in val_loop:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / total_val\n",
    "        val_accuracy = val_correct / total_val\n",
    "\n",
    "        precision = precision_score(all_labels, all_preds, average='binary')\n",
    "        recall = recall_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"  Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_accuracy:.4f}\")\n",
    "        print(f\"  Precision:  {precision:.4f} | Recall:    {recall:.4f}\")\n",
    "        print(\"-\" * 50)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:07:55.251743300Z",
     "start_time": "2025-04-28T12:07:55.243104600Z"
    }
   },
   "id": "60c220b593104b77"
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "backbone_params = [param for param in model.features.parameters() if param.requires_grad]\n",
    "head_params = [param for param in model.classifier.parameters()]\n",
    "\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam([\n",
    "#     {'params': backbone_params, 'lr': 1e-3}, \n",
    "#     {'params': head_params, 'lr': 1e-3}],)\n",
    "#     # weight_decay=1e-5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr,)\n",
    "    # weight_decay=1e-4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:07:55.697777900Z",
     "start_time": "2025-04-28T12:07:55.692618300Z"
    }
   },
   "id": "6b2b4efdc883e50a"
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for img, idx in train_loader:\n",
    "    print(img.shape, idx.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:07:56.322669100Z",
     "start_time": "2025-04-28T12:07:56.241955200Z"
    }
   },
   "id": "bd353e7f4c36f441"
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ExplainableFaceMorph\\code\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "  Train Loss: 0.6952 | Train Acc: 0.4969\n",
      "  Val Loss:   0.7002 | Val Acc:   0.5122\n",
      "  Precision:  0.0000 | Recall:    0.0000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ExplainableFaceMorph\\code\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20]\n",
      "  Train Loss: 0.6866 | Train Acc: 0.5706\n",
      "  Val Loss:   0.6959 | Val Acc:   0.5122\n",
      "  Precision:  0.0000 | Recall:    0.0000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ExplainableFaceMorph\\code\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20]\n",
      "  Train Loss: 0.6842 | Train Acc: 0.5706\n",
      "  Val Loss:   0.6933 | Val Acc:   0.5122\n",
      "  Precision:  0.0000 | Recall:    0.0000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20]\n",
      "  Train Loss: 0.6769 | Train Acc: 0.5890\n",
      "  Val Loss:   0.6961 | Val Acc:   0.4878\n",
      "  Precision:  0.4878 | Recall:    1.0000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20]\n",
      "  Train Loss: 0.6671 | Train Acc: 0.6687\n",
      "  Val Loss:   0.6994 | Val Acc:   0.4634\n",
      "  Precision:  0.4750 | Recall:    0.9500\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20]\n",
      "  Train Loss: 0.6561 | Train Acc: 0.6442\n",
      "  Val Loss:   0.7220 | Val Acc:   0.4878\n",
      "  Precision:  0.4706 | Recall:    0.4000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20]\n",
      "  Train Loss: 0.6420 | Train Acc: 0.7178\n",
      "  Val Loss:   0.7268 | Val Acc:   0.4390\n",
      "  Precision:  0.0000 | Recall:    0.0000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20]\n",
      "  Train Loss: 0.6201 | Train Acc: 0.7730\n",
      "  Val Loss:   0.7662 | Val Acc:   0.3659\n",
      "  Precision:  0.2000 | Recall:    0.1000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20]\n",
      "  Train Loss: 0.5788 | Train Acc: 0.7669\n",
      "  Val Loss:   0.7678 | Val Acc:   0.4390\n",
      "  Precision:  0.4545 | Recall:    0.7500\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20]\n",
      "  Train Loss: 0.5348 | Train Acc: 0.8712\n",
      "  Val Loss:   0.7520 | Val Acc:   0.4390\n",
      "  Precision:  0.4286 | Recall:    0.4500\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20]\n",
      "  Train Loss: 0.4555 | Train Acc: 0.9080\n",
      "  Val Loss:   0.8629 | Val Acc:   0.4390\n",
      "  Precision:  0.3846 | Recall:    0.2500\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20]\n",
      "  Train Loss: 0.4169 | Train Acc: 0.9141\n",
      "  Val Loss:   0.8486 | Val Acc:   0.3902\n",
      "  Precision:  0.4242 | Recall:    0.7000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20]\n",
      "  Train Loss: 0.3332 | Train Acc: 0.9632\n",
      "  Val Loss:   0.8311 | Val Acc:   0.4146\n",
      "  Precision:  0.4231 | Recall:    0.5500\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20]\n",
      "  Train Loss: 0.3084 | Train Acc: 0.9571\n",
      "  Val Loss:   0.9326 | Val Acc:   0.3415\n",
      "  Precision:  0.4000 | Recall:    0.7000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20]\n",
      "  Train Loss: 0.2174 | Train Acc: 0.9877\n",
      "  Val Loss:   1.1014 | Val Acc:   0.3659\n",
      "  Precision:  0.4167 | Recall:    0.7500\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20]\n",
      "  Train Loss: 0.1692 | Train Acc: 0.9816\n",
      "  Val Loss:   0.9776 | Val Acc:   0.4878\n",
      "  Precision:  0.4706 | Recall:    0.4000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ExplainableFaceMorph\\code\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20]\n",
      "  Train Loss: 0.1571 | Train Acc: 0.9877\n",
      "  Val Loss:   1.3797 | Val Acc:   0.5122\n",
      "  Precision:  0.0000 | Recall:    0.0000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20]\n",
      "  Train Loss: 0.1550 | Train Acc: 0.9755\n",
      "  Val Loss:   0.9742 | Val Acc:   0.4634\n",
      "  Precision:  0.4444 | Recall:    0.4000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20]\n",
      "  Train Loss: 0.0918 | Train Acc: 0.9939\n",
      "  Val Loss:   1.3442 | Val Acc:   0.4634\n",
      "  Precision:  0.4000 | Recall:    0.2000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20]\n",
      "  Train Loss: 0.0746 | Train Acc: 1.0000\n",
      "  Val Loss:   1.4244 | Val Acc:   0.4390\n",
      "  Precision:  0.4615 | Recall:    0.9000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, test_loader, criterion, optimizer, DEVICE, num_epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:08:12.056637700Z",
     "start_time": "2025-04-28T12:07:56.939350900Z"
    }
   },
   "id": "aeaa25c7a9aed39f"
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import torchvision.transforms as transforms\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# \n",
    "# # Helper function to preprocess a single image\n",
    "# def preprocess_image(img_path):\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # adjust if needed\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "#                              std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "#     from PIL import Image\n",
    "#     img = Image.open(img_path).convert('RGB')\n",
    "#     return transform(img).unsqueeze(0)  # Add batch dimension\n",
    "# \n",
    "# # Hook to capture gradients and activations\n",
    "# class ActivationsAndGradients:\n",
    "#     def __init__(self, model, target_layer):\n",
    "#         self.model = model\n",
    "#         self.target_layer = target_layer\n",
    "#         self.gradients = None\n",
    "#         self.activations = None\n",
    "# \n",
    "#         self.hook_a = self.target_layer.register_forward_hook(self.save_activation)\n",
    "#         self.hook_g = self.target_layer.register_full_backward_hook(self.save_gradient)  # <- FULL backward hook only\n",
    "# \n",
    "#     def save_activation(self, module, input, output):\n",
    "#         self.activations = output\n",
    "# \n",
    "#     def save_gradient(self, module, grad_input, grad_output):\n",
    "#         self.gradients = grad_output[0]\n",
    "# \n",
    "#     def remove(self):\n",
    "#         self.hook_a.remove()\n",
    "#         self.hook_g.remove()\n",
    "# \n",
    "# # GradCAM computation\n",
    "# def compute_gradcam(model, image_tensor, target_layer, class_idx=None):\n",
    "#     model.eval()\n",
    "#     device = next(model.parameters()).device\n",
    "#     image_tensor = image_tensor.to(device)\n",
    "# \n",
    "#     hook = ActivationsAndGradients(model, target_layer)\n",
    "# \n",
    "#     output = model(image_tensor)  # Forward pass\n",
    "#     if class_idx is None:\n",
    "#         class_idx = output.argmax(dim=1).item()\n",
    "# \n",
    "#     loss = output[:, class_idx]\n",
    "#     model.zero_grad()\n",
    "#     loss.backward()\n",
    "# \n",
    "#     # Get gradients and activations\n",
    "#     gradients = hook.gradients  # [B, C, H, W]\n",
    "#     activations = hook.activations  # [B, C, H, W]\n",
    "#     hook.remove()\n",
    "# \n",
    "#     # Compute weights: global average pooling the gradients\n",
    "#     weights = gradients.mean(dim=(2, 3), keepdim=True)  # [B, C, 1, 1]\n",
    "#     gradcam_map = (weights * activations).sum(dim=1, keepdim=True)  # [B, 1, H, W]\n",
    "#     gradcam_map = F.relu(gradcam_map)\n",
    "# \n",
    "#     # Normalize\n",
    "#     gradcam_map = F.interpolate(gradcam_map, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "#     gradcam_map = gradcam_map.squeeze().detach().cpu().numpy()\n",
    "#     gradcam_map = (gradcam_map - gradcam_map.min()) / (gradcam_map.max() - gradcam_map.min() + 1e-8)\n",
    "# \n",
    "#     return gradcam_map\n",
    "# \n",
    "# # Function to visualize GradCAM overlay\n",
    "# def show_gradcam_on_image(img_path, gradcam_map, alpha=0.5):\n",
    "#     img = cv2.imread(img_path)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     img = cv2.resize(img, (224, 224))\n",
    "# \n",
    "#     heatmap = cv2.applyColorMap(np.uint8(255 * gradcam_map), cv2.COLORMAP_JET)\n",
    "#     heatmap = np.float32(heatmap) / 255\n",
    "# \n",
    "#     cam = heatmap + np.float32(img) / 255\n",
    "#     cam = cam / np.max(cam)\n",
    "# \n",
    "#     plt.imshow(cam)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:06:33.209092700Z",
     "start_time": "2025-04-28T12:06:33.204092200Z"
    }
   },
   "id": "1577348a6524413c"
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "# # # Load your model\n",
    "# # model = torch.hub.load(f\"{MODEL_DIR}/pytorch_vision_v0.10.0\", 'mobilenet_v2', source=\"local\")\n",
    "# # model.classifier[1] = torch.nn.Linear(1280, 2)  # your modification\n",
    "# model.eval()\n",
    "# \n",
    "# # Move to GPU if available\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "# \n",
    "# # Load an image\n",
    "# img_path = f\"{DATA_DIR}/fake/001_08_q32.jpg\"\n",
    "# img_tensor = preprocess_image(img_path)\n",
    "# \n",
    "# # Pick a target layer manually\n",
    "# target_layer = model.features[1]  # Example: you can pick any layer like 3, 5, 13, etc.\n",
    "# \n",
    "# # Compute GradCAM\n",
    "# gradcam_map = compute_gradcam(model, img_tensor, target_layer)\n",
    "# \n",
    "# # Show GradCAM\n",
    "# show_gradcam_on_image(img_path, gradcam_map)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:06:33.214110400Z",
     "start_time": "2025-04-28T12:06:33.207092900Z"
    }
   },
   "id": "47264f7f52a86d4c"
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:06:33.214110400Z",
     "start_time": "2025-04-28T12:06:33.213606400Z"
    }
   },
   "id": "e9a2d68765f23588"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
